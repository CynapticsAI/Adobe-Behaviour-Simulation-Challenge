{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_cK83Ribpri9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import  numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_format(prompt,system_message):\n",
        "  return(f\"\"\"<|im_start|>system\n",
        "- You answer questions.\n",
        "- You are more than just an information source, you are also able to write poetry, short stories, and make jokes\n",
        "{system_message}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt}<|im_end|>\n",
        "<|im_start|>assistant\"\"\")\n",
        "\n",
        "\n",
        "#   <|im_start|>system\n",
        "# .<|im_end|>\n",
        "# <|im_start|>user\n",
        "# Explain QKV<|im_end|>\n",
        "# <|im_start|>assistant\n"
      ],
      "metadata": {
        "id": "iNicYbp0rNOE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://huggingface.co/TheBloke/una-cybertron-7B-v2-GGUF/blob/main/una-cybertron-7b-v2-bf16.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "CwVnE7W3rVpM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/una-cybertron-7B-v2-GGUF una-cybertron-7b-v2-bf16.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "id": "V1mBBFOozSrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580483fb-3772-499a-941c-ea0213fa640f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/una-cybertron-7B-v2-GGUF/resolve/main/una-cybertron-7b-v2-bf16.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp7_rfxn85\n",
            "una-cybertron-7b-v2-bf16.Q4_K_M.gguf: 100% 4.37G/4.37G [00:40<00:00, 108MB/s]\n",
            "./una-cybertron-7b-v2-bf16.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://huggingface.co/TheBloke/una-cybertron-7B-v2-GGUF/blob/main/una-cybertron-7b-v2-bf16.Q5_0.gguf"
      ],
      "metadata": {
        "id": "zF9j5S0TrgLW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "gwbbzbzErj4O",
        "outputId": "35c62318-80bf-4fbb-909b-c7949c08d79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1bb217d7-0f13-4317-abfc-aa700ae5f598\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1bb217d7-0f13-4317-abfc-aa700ae5f598\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!kaggle datasets download -d xenon/bst-300k\n",
        "!kaggle datasets download -d xenon/similarity2\n",
        "!unzip \"/content/similarity2.zip\" -d \"/content/similarity2\""
      ],
      "metadata": {
        "id": "qQJyUGrWrpnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/bst-300k.zip\" -d \"/content/data\""
      ],
      "metadata": {
        "id": "RgmRmgnzrp5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv(\"/content/data/behaviour_simulation_train.csv\")\n",
        "similarity=pd.read_csv(\"/content/similarity2/final.csv\")"
      ],
      "metadata": {
        "id": "JOd-61GKr0BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIqObH3QxxGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweeterpy\n",
        "!pip install cohere\n",
        "!pip install gradio_client\n",
        "!pip install pinecone-client langchain sentence-transformers\n",
        "!pip install Wikipedia-API"
      ],
      "metadata": {
        "id": "p47wWfNSr0fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997047cc-76e9-4e7d-d1e8-f8bf7fcde6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y\n",
            "Requirement already satisfied: tweeterpy in /usr/local/lib/python3.10/dist-packages (1.0.13)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.2 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (4.12.2)\n",
            "Requirement already satisfied: Brotli==1.0.9 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (1.0.9)\n",
            "Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (0.0.1)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer==3.1.0 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (3.1.0)\n",
            "Requirement already satisfied: demjson3==3.0.6 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (3.0.6)\n",
            "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (3.4)\n",
            "Requirement already satisfied: lxml==4.9.2 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (4.9.2)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (2.31.0)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (1.16.0)\n",
            "Requirement already satisfied: soupsieve==2.4.1 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (2.4.1)\n",
            "Requirement already satisfied: urllib3==2.0.7 in /usr/local/lib/python3.10/dist-packages (from tweeterpy) (2.0.7)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.37)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.1)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.7.22)\n",
            "Requirement already satisfied: gradio_client in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio_client) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio_client) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from gradio_client) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio_client) (23.2)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio_client) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio_client) (4.5.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio_client) (11.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio_client) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio_client) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio_client) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio_client) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio_client) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio_client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio_client) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->gradio_client) (0.17.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio_client) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio_client) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio_client) (3.7.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio_client) (1.2.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.347)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.5.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.11)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.69)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: Wikipedia-API in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wikipediaapi import Wikipedia\n",
        "from tweeterpy import TweeterPy\n",
        "from tweeterpy.util import User, Tweet\n",
        "twitter = TweeterPy()"
      ],
      "metadata": {
        "id": "njeo5Qklr6GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_average_likes(input):\n",
        "    try:\n",
        "        tweet_info = data.iloc[input]\n",
        "        user_name = tweet_info['username']\n",
        "        user = twitter.get_user_data(user_name)\n",
        "        favourites_count = user['legacy']['favourites_count']\n",
        "        media_count = user['legacy']['media_count']\n",
        "        return favourites_count / media_count\n",
        "    except Exception as e:\n",
        "        return \"Couldnt find a value\"  # You can return a default value or handle it according to your needs\n",
        "# k <=15,index corrresponds to 0-9999 respectively for 290000-300000\n",
        "def RagTopK(k,index):\n",
        "    a=similarity.iloc[index].tolist()\n",
        "    f=[]\n",
        "    for i in range(0,len(a)-1,2):\n",
        "        mini=[a[i],a[i+1]]\n",
        "        f.append(mini)\n",
        "    sorted_list = sorted(f, key=lambda x: -x[1])\n",
        "    return sorted_list[:k]\n",
        "\n",
        "def compiletweets(input):\n",
        "    string=\"\"\n",
        "    for li in input:\n",
        "        tweet_info=data.iloc[int(li[0])]\n",
        "\n",
        "        formatted_string = (\n",
        "            f\"On {tweet_info['date']}, under the username {tweet_info['username']}, \"\n",
        "            f\"posted a tweet with the content: \\\"{tweet_info['content']}\\\". \"\n",
        "            f\"The tweet received {tweet_info['likes']} likes.\"\n",
        "        )\n",
        "        string=string+formatted_string+\", \\n\"\n",
        "    return string\n",
        "\n",
        "def stringifyInput(input):\n",
        "    tweet_info=data.iloc[input]\n",
        "    formatted_string = (\n",
        "            f\"On {tweet_info['date']}, under the username {tweet_info['username']}, \"\n",
        "            f\"posted a tweet with the content: \\\"{tweet_info['content']}\\\". \"\n",
        "            f\"Predict The number of likes it will recieve.output a number i do not want any explaination,again please output just the number of likes.\"\n",
        "        )\n",
        "    return formatted_string\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u3ejh9bzsAmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chainlit\n",
        "!pip install torch\n",
        "!pip install langchain\n",
        "!pip install ctransformers[cuda]\n",
        "!pip install gguf\n",
        "\n",
        "\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ccvnc-mgsAyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama_cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VutcZ0QFtF38",
        "outputId": "953f924a-9369-4343-e0e5-56bf03354bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.20)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chainlit as cl\n",
        "from langchain.llms import CTransformers\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import LlamaCpp\n",
        "import torch\n",
        "import gguf\n",
        "import re\n",
        "from ctransformers import AutoModelForCausalLM,AutoConfig\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "QpFWVG1hslqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub\n"
      ],
      "metadata": {
        "id": "0pLu2Gvfwbt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = Llama(\n",
        "  model_path=\"/content/una-cybertron-7b-v2-bf16.Q4_K_M.gguf\",  # Download the model file first\n",
        "  n_ctx=8192,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
        "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
        "  n_gpu_layers=400         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6vNDlR3so11",
        "outputId": "01a3f303-396e-495b-d3d2-374b8188f812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k <=15,index corrresponds to 0-9999 respectively for 290000-300000\n",
        "def RagTopK(k,index):\n",
        "    a=similarity.iloc[index].tolist()\n",
        "    f=[]\n",
        "    for i in range(0,len(a)-1,2):\n",
        "        mini=[a[i],a[i+1]]\n",
        "        f.append(mini)\n",
        "    sorted_list = sorted(f, key=lambda x: -x[1])\n",
        "    return sorted_list[:k]\n",
        "\n",
        "def compiletweets(input):\n",
        "    string=\"\"\n",
        "    for li in input:\n",
        "        tweet_info=data.iloc[int(li[0])]\n",
        "\n",
        "        formatted_string = (\n",
        "            f\"On {tweet_info['date']}, under the username {tweet_info['username']}, \"\n",
        "            f\"posted a tweet with the content: \\\"{tweet_info['content']}\\\". \"\n",
        "            f\"The tweet received {tweet_info['likes']} likes.\"\n",
        "        )\n",
        "        string=string+formatted_string+\", \\n\"\n",
        "    return string\n",
        "\n",
        "def stringifyInput(input):\n",
        "    tweet_info=data.iloc[input]\n",
        "    formatted_string = (\n",
        "            f\"On {tweet_info['date']}, under the username {tweet_info['username']}, \"\n",
        "            f\"posted a tweet with the content: \\\"{tweet_info['content']}\\\". \"\n",
        "            f\"Predict The number of likes it will recieve.output a number i do not want any explaination,again please output just the number of likes.\"\n",
        "        )\n",
        "    return formatted_string\n",
        "\n",
        "\n",
        "\n",
        "from wikipediaapi import Wikipedia\n",
        "\n",
        "def get_company_page_content(input):\n",
        "    try:\n",
        "        tweet_info = data.iloc[input]\n",
        "        company_name = tweet_info['inferred company']\n",
        "\n",
        "        # Check if the company_name is \"independent\"\n",
        "        if company_name.lower() == \"independent\":\n",
        "\n",
        "            return f\"No information found for {company_name}\"\n",
        "\n",
        "        # Use Wikipedia API to get the page content\n",
        "        wiki_wiki = Wikipedia('JeremyHowardBot/0.0', 'en')\n",
        "        page = wiki_wiki.page(company_name)\n",
        "\n",
        "        # Check if the page exists\n",
        "        if page.exists():\n",
        "            # Get the page content and remove the 'References' section\n",
        "            page_content = page.text.split('\\nReferences\\n')[0]\n",
        "            return page_content\n",
        "        else:\n",
        "            return f\"No information found for {company_name}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"Couldnt find any relevant data\"  # You can return a default value or handle it according to your needs\n",
        "def get_average_likes(input):\n",
        "    try:\n",
        "        tweet_info = data.iloc[input]\n",
        "        user_name = tweet_info['username']\n",
        "        user = twitter.get_user_data(user_name)\n",
        "        favourites_count = user['legacy']['favourites_count']\n",
        "        media_count = user['legacy']['media_count']\n",
        "        return favourites_count / media_count\n",
        "    except Exception as e:\n",
        "        return \"Couldnt find a value\"  # You can return a default value or handle it according to your needs\n"
      ],
      "metadata": {
        "id": "Sxc73n24wrpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_prompt = \"\"\"\n",
        "You are a good model you can only output a number,Given a set of Twitter tweets with information about id, tweet content, date, username, company, and likes, your task is to analyze these tweets and predict the exact number of likes a new tweet will receive. The input will include details of previous tweets in the format:\n",
        "\n",
        "    Tweet ID: [id], Content: [tweet], Date: [date], Username: [username], Company: [company], Likes: [likes]\n",
        "\n",
        "    You will also be given the company's average likes accross all its tweets and also the company's general information. the company in context here is the company tweeting the tweet whose likes you have to predict\n",
        "\n",
        "    Please use the provided information to make predictions about the number of likes a new tweet will receive. Output only the predicted number of likes for the new tweet. Avoid providing explanations or additional information in the output.\n",
        "\"\"\"\n",
        "instruction_prompt2=\"\"\"\n",
        "Predict the likes for a new tweet using this Twitter dataset. Each entry follows the format:\n",
        "\n",
        "Tweet ID: [id], Content: [tweet], Date: [date], Username: [username], Company: [company], Likes: [likes]\n",
        "\n",
        "You will also be given the company's general description and also the average number of likes the company gets on its tweets.\n",
        "You will also be given the audio and image captioning for the media of the tweet.\n",
        "Using this Output only the predicted number of likes,  Do not include any explanations or additional details, provide the numerical prediction only\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "instruction_prompt3 = \"\"\"\n",
        "You are a good model you can only output a number,Given a set of Twitter tweets with information about id, tweet content, date, username, company, and likes, your task is to analyze these tweets and predict the exact number of likes a new tweet will receive. The input will include details of previous tweets in the format:\n",
        "\n",
        "    Tweet ID: [id], Content: [tweet], Date: [date], Username: [username], Company: [company], Likes: [likes]\n",
        "\n",
        "    You will also be given the company's average likes accross all its tweets and also the company's general information. the company in context here is the company tweeting the tweet whose likes you have to predict\n",
        "\n",
        "    Please use the provided information to make predictions about the number of likes a new tweet will receive. Output only the predicted number of likes for the new tweet. Avoid providing explanations or additional information in the output.\n",
        "    if youre not able to predict the likes , then simply output the average number of likes of the company\n",
        "    Output should be of the form:\n",
        "    Prediction:(only an integer)\n",
        "    No explaination or any procedure required,only give me the predictions.Thank You.\n",
        "\"\"\"\n",
        "\n",
        "def input_prompt(k_rag ,index ):\n",
        "  similar_tweet_1 = RagTopK(k_rag ,index)\n",
        "  similar_tweet = compiletweets(similar_tweet_1)\n",
        "  average_likes=get_average_likes(290000+index)\n",
        "  # average_likes=\"null\"\n",
        "  # company_content=\"null\"\n",
        "  company_content = get_company_page_content(290000+index)\n",
        "  wikidata_company = company_content[:200]\n",
        "  # wikidata_company=\"null\"\n",
        "  image_captioning =\"null\"\n",
        "  audio_captioning =\"null\"\n",
        "  tweet= stringifyInput(290000+index)\n",
        "  prompt = instruction_prompt3 + \"Here are the similar tweets: \" + similar_tweet + \"The company has \" + str(average_likes) +\" average likes accross its tweets.\"+\"The company's general description is this :\"+wikidata_company+\"The tweet is this :\"+tweet+\"The image captioning of the image in the tweet is :\"+image_captioning +\"The audio captioning of the audio in the tweet is :\"+audio_captioning\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "RfUn-u6-wNos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enter=100#Enter index of the tweet whose likes you want to predict(you can predict only for index>=290000 so if you want results of index 290000 enter will be 0,for 300000 enter will be 9999)\n",
        "input = input_prompt ( 5 , enter)\n",
        "acctual_likes = data.iloc[290000+enter]['likes']\n",
        "print(acctual_likes)\n",
        "# prediction_number = int(match.group())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKW2OWACwQgw",
        "outputId": "600ecc66-d280-4713-aab9-c6ba7dde9d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-07 20:12:23,348 [\u001b[0;33mWARNING\u001b[0m] :: Couldn't generate features for request variables.\n",
            "2023-12-07 20:12:23,486 [\u001b[0;32mINFO\u001b[0m] :: Wikipedia: language=en, user_agent: JeremyHowardBot/0.0 (Wikipedia-API/0.6.0; https://github.com/martin-majlis/Wikipedia-API/), extract_format=ExtractFormat.WIKI\n",
            "2023-12-07 20:12:23,488 [\u001b[0;32mINFO\u001b[0m] :: Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=info&titles=cameron&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle\n",
            "2023-12-07 20:12:23,790 [\u001b[0;32mINFO\u001b[0m] :: Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Cameron&explaintext=1&exsectionformat=wiki\n",
            "1396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VD9cJCLwxuA",
        "outputId": "539c6810-055b-41de-bd0e-1e133968927b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a good model you can only output a number,Given a set of Twitter tweets with information about id, tweet content, date, username, company, and likes, your task is to analyze these tweets and predict the exact number of likes a new tweet will receive. The input will include details of previous tweets in the format:\n",
            "\n",
            "    Tweet ID: [id], Content: [tweet], Date: [date], Username: [username], Company: [company], Likes: [likes]\n",
            "\n",
            "    You will also be given the company's average likes accross all its tweets and also the company's general information. the company in context here is the company tweeting the tweet whose likes you have to predict\n",
            "\n",
            "    Please use the provided information to make predictions about the number of likes a new tweet will receive. Output only the predicted number of likes for the new tweet. Avoid providing explanations or additional information in the output.\n",
            "    if youre not able to predict the likes , then simply output the average number of likes of the company\n",
            "    Output should be of the form:\n",
            "    Prediction:(only an integer)\n",
            "    No explaination or any procedure required,only give me the predictions.Thank You.\n",
            "Here are the similar tweets: On 2020-05-15 06:07:40, under the username ScooterMagruder, posted a tweet with the content: \"One gotta go: <hyperlink>\". The tweet received 233 likes., \n",
            "On 2020-05-16 00:56:06, under the username ScooterMagruder, posted a tweet with the content: \"One gotta go: <hyperlink>\". The tweet received 378 likes., \n",
            "On 2020-05-23 00:44:50, under the username ScooterMagruder, posted a tweet with the content: \"One gotta go: <hyperlink>\". The tweet received 138 likes., \n",
            "On 2020-05-18 20:28:03, under the username ScooterMagruder, posted a tweet with the content: \"One gotta go: <hyperlink>\". The tweet received 254 likes., \n",
            "On 2020-05-19 20:01:58, under the username ScooterMagruder, posted a tweet with the content: \"One gotta go: <hyperlink>\". The tweet received 169 likes., \n",
            "The company has 1.7212408444635934 average likes accross its tweets.The company's general description is this :Cameron may refer to:\n",
            "\n",
            "People\n",
            "Clan Cameron, a Scottish clan\n",
            "Cameron (given name), a given name (including a list of people with the name)\n",
            "Cameron (surname), a surname (including a list of people with The tweet is this :On 2019-09-04 14:20:29, under the username ScooterMagruder, posted a tweet with the content: \"Yall see where I get it from... <hyperlink>\". Predict The number of likes it will recieve.output a number i do not want any explaination,again please output just the number of likes.The image captioning of the image in the tweet is :nullThe audio captioning of the audio in the tweet is :null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getResult(input):\n",
        "  return llm(\n",
        "    prompt_format(input,\"You are a Like predictor model,Given tweets with some more relevant information you can output the number of likes a new tweet can predict. \"), # Prompt\n",
        "    max_tokens=512,  # Generate up to 512 tokens\n",
        "    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
        "    echo=True        # Whether to echo the prompt\n",
        "  )\n",
        "\n",
        "# Chat Completion API\n",
        "\n"
      ],
      "metadata": {
        "id": "QJqqAfDMuRQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actuals=[]\n",
        "preds=[]\n",
        "\n",
        "for i in range(0,100):\n",
        "  enter=i +1000 #Enter index of the tweet whose likes you want to predict(you can predict only for index>=290000 so if you want results of index 290000 enter will be 0,for 300000 enter will be 9999)\n",
        "  input = input_prompt ( 5 , enter)\n",
        "  output=getResult(input)\n",
        "  input_string=output['choices'][0]['text']\n",
        "  print(input_string)\n",
        "  # match = re.search(r'\\d+$', input_string)\n",
        "  # prediction_number = int(match.group()) if match else 770\n",
        "  match = re.search(r'Prediction:\\s*(\\d+)', input_string)\n",
        "\n",
        "# Check if a match is found and get the number\n",
        "  prediction_number = int(match.group(1)) if match else None\n",
        "\n",
        "  # prediction_number = int(re.search(r'\\d+$', input_string).group())\n",
        "  print(prediction_number)\n",
        "  acctual_likes = data.iloc[290000+enter]['likes']\n",
        "  print(f\"Actual likes {acctual_likes}\")\n",
        "  actuals.append(acctual_likes)\n",
        "  preds.append(prediction_number)\n",
        "\n"
      ],
      "metadata": {
        "id": "qmBFMQWWvxzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# preds=[]\n",
        "# actual=[]\n",
        "# total_inputs = 0\n",
        "output_file_path = \"/content/gdrive/MyDrive/INTERIIT_MIDPREP/likes_predictions_una.csv\"\n",
        "with open(output_file_path, 'w', newline='') as csvfile:\n",
        "  csv_writer = csv.writer(csvfile)\n",
        "  csv_writer.writerow(['Actual Likes', 'Prediction Number'])\n",
        "\n",
        "\n",
        "\n",
        "actuals=[]\n",
        "preds=[]\n",
        "\n",
        "for i in range(0,10000):\n",
        "  print(i)\n",
        "  enter=i +1000 #Enter index of the tweet whose likes you want to predict(you can predict only for index>=290000 so if you want results of index 290000 enter will be 0,for 300000 enter will be 9999)\n",
        "  input = input_prompt ( 5 , enter)\n",
        "  output=getResult(input)\n",
        "  input_string=output['choices'][0]['text']\n",
        "  # print(input_string)\n",
        "  # match = re.search(r'\\d+$', input_string)\n",
        "  # prediction_number = int(match.group()) if match else 770\n",
        "  match = re.search(r'Prediction:\\s*(\\d+)', input_string)3z\n",
        "\n",
        "# Check if a match is found and get the number\n",
        "  prediction_number = int(match.group(1)) if match else 770\n",
        "\n",
        "  # prediction_number = int(re.search(r'\\d+$', input_string).group())\n",
        "  # print(prediction_number)\n",
        "  acctual_likes = data.iloc[290000+enter]['likes']\n",
        "  # print(f\"Actual likes {acctual_likes}\")\n",
        "  actuals.append(acctual_likes)\n",
        "  preds.append(prediction_number)\n",
        "\n",
        "  with open(output_file_path, 'a', newline='') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow([data.iloc[290000 + enter]['likes'], prediction_number])\n",
        "\n"
      ],
      "metadata": {
        "id": "dxK3LHqUOiWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "rmse=0\n",
        "for i in range(0,len(preds)):\n",
        "  rmse=rmse+(actuals[i]-preds[i])**2\n",
        "rmse=math.sqrt(rmse/len(preds))\n",
        "print(f\"rmse is {rmse}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFv-Kjah9m07",
        "outputId": "b474976a-ecfa-4874-981e-f6bbe115f169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse is 4610.233249100431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=\"./una-cybertron-7b-v2-bf16.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
        "llm.create_chat_completion(\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a story about llamas.\"\n",
        "        }\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "MElg_tOvz06a",
        "outputId": "3bfaa5ef-2ec1-49fc-af1f-0ec63f114e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-9f242265252e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./una-cybertron-7b-v2-bf16.Q4_K_M.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-2\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set chat_format according to the model you are using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m llm.create_chat_completion(\n\u001b[0m\u001b[1;32m      3\u001b[0m     messages = [\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"You are a story writing assistant.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   2089\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m         )\n\u001b[0;32m-> 2091\u001b[0;31m         return handler(\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0mllama\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama_chat_format.py\u001b[0m in \u001b[0;36mbasic_create_chat_completion\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             completion_or_chunks = llama.create_completion(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1927\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m   1463\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m             token = self.sample(\n\u001b[1;32m   1239\u001b[0m                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             )\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama_cpp.py\u001b[0m in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfind\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKV\u001b[0m \u001b[0mslot\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mtry\u001b[0m \u001b[0mreducing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mincrease\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     < 0 - error\"\"\"\n\u001b[0;32m-> 1433\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllama_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMaPFR_2thVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}