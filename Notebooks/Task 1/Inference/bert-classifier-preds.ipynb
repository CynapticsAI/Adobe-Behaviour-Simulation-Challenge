{"cells":[{"cell_type":"markdown","metadata":{},"source":["Upgrading the package"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install --upgrade scikit-learn"]},{"cell_type":"markdown","metadata":{},"source":["Importing Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T12:49:58.135870Z","iopub.status.busy":"2023-12-14T12:49:58.135479Z","iopub.status.idle":"2023-12-14T12:49:58.166590Z","shell.execute_reply":"2023-12-14T12:49:58.165581Z","shell.execute_reply.started":"2023-12-14T12:49:58.135839Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import joblib\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["Loading the Model"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T12:49:58.627826Z","iopub.status.busy":"2023-12-14T12:49:58.627445Z","iopub.status.idle":"2023-12-14T12:50:00.352507Z","shell.execute_reply":"2023-12-14T12:50:00.350353Z","shell.execute_reply.started":"2023-12-14T12:49:58.627796Z"},"trusted":true},"outputs":[],"source":["# load this model from  Bertweet-classifier-model from kaggle dataset\n","clf=joblib.load(\"/path/training/random_forest_model_bertTweet(290k)_1.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["Loading the  Bertweet Embeddings"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.353583Z","iopub.status.idle":"2023-12-14T12:50:00.354216Z","shell.execute_reply":"2023-12-14T12:50:00.353995Z","shell.execute_reply.started":"2023-12-14T12:50:00.353971Z"},"trusted":true},"outputs":[],"source":["# load the Adobe-test-embeddings here\n","input_directory = \"/path/to/file/archive (4)\"\n","npy_files = glob.glob(os.path.join(input_directory, '*.npy'))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.355988Z","iopub.status.idle":"2023-12-14T12:50:00.356876Z","shell.execute_reply":"2023-12-14T12:50:00.356679Z","shell.execute_reply.started":"2023-12-14T12:50:00.356657Z"},"trusted":true},"outputs":[],"source":["npy_files.sort()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.358389Z","iopub.status.idle":"2023-12-14T12:50:00.358820Z","shell.execute_reply":"2023-12-14T12:50:00.358630Z","shell.execute_reply.started":"2023-12-14T12:50:00.358610Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['/path/to/file/archive (4)/content-test-company-date.npy',\n"," '/path/to/file/archive (4)/content-test-company-inferred-company.npy',\n"," '/path/to/file/archive (4)/content-test-company-username.npy',\n"," '/path/to/file/archive (4)/content-test-company.npy',\n"," '/path/to/file/archive (4)/content-test-time-date.npy',\n"," '/path/to/file/archive (4)/content-test-time-inferred-company.npy',\n"," '/path/to/file/archive (4)/content-test-time-username.npy',\n"," '/path/to/file/archive (4)/content-test-time.npy',\n"," '/path/to/file/archive (4)/full-content-test-company.npy',\n"," '/path/to/file/archive (4)/full-content-test-time.npy']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["npy_files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#  in the next cell we are loading all of the individual embeddings"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.360115Z","iopub.status.idle":"2023-12-14T12:50:00.360553Z","shell.execute_reply":"2023-12-14T12:50:00.360369Z","shell.execute_reply.started":"2023-12-14T12:50:00.360349Z"},"trusted":true},"outputs":[],"source":["# for company data\n","# Load each .npy file and concatenate them\n","arrays = [np.load(file) for file in npy_files[3:4]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_text = pd.DataFrame(combined_array, columns=column_names)\n","\n","arrays = [np.load(file) for file in npy_files[0:1]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_date = pd.DataFrame(combined_array, columns=column_names)\n","\n","arrays = [np.load(file) for file in npy_files[1:2]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_com = pd.DataFrame(combined_array, columns=column_names)\n","\n","arrays = [np.load(file) for file in npy_files[2:3]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_usr = pd.DataFrame(combined_array, columns=column_names)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.363227Z","iopub.status.idle":"2023-12-14T12:50:00.363922Z","shell.execute_reply":"2023-12-14T12:50:00.363729Z","shell.execute_reply.started":"2023-12-14T12:50:00.363708Z"},"trusted":true},"outputs":[],"source":["# defining a function to get predictions\n","\n","def predict_likes_class(model, index,text,date,com,usr):\n","    # Create a DataFrame with the input features\n","    input_data = pd.DataFrame({\n","        'content': [],\n","        'inferred company': [],\n","        'date': [],\n","        'username': []\n","    })\n","\n","    # Get word embeddings for the 'Content' column\n","    content_embedding = text[index:index+1]\n","    usr_embedding = usr[index:index+1]\n","    com_embedding = com[index:index+1]\n","    date_embedding = date[index:index+1]\n","\n","    # Combine embeddings with other features\n","    input_combined = pd.concat([input_data.drop('content', axis=1), content_embedding], axis=1)\n","    input_combined = pd.concat([input_combined.drop('date', axis=1), date_embedding], axis=1)\n","    input_combined = pd.concat([input_combined.drop('inferred company', axis=1), com_embedding], axis=1)\n","    input_combined = pd.concat([input_combined.drop('username', axis=1), usr_embedding], axis=1)\n","\n","    # Make predictions\n","    prediction = model.predict(input_combined)\n","\n","    return prediction"]},{"cell_type":"markdown","metadata":{},"source":["Predicting for unseen company"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.365024Z","iopub.status.idle":"2023-12-14T12:50:00.365801Z","shell.execute_reply":"2023-12-14T12:50:00.365582Z","shell.execute_reply.started":"2023-12-14T12:50:00.365548Z"},"trusted":true},"outputs":[],"source":["ac=[]\n","ind=[]\n","for index in range(0,10000):\n","    predicted_class = predict_likes_class(clf, index,df_text,df_date,df_com,df_usr)\n","    ind.append(index)\n","    ac.append(predicted_class[0])\n","ak2=pd.DataFrame({\"index\":ind,\"classByBertTweetClassifier_company\":ac})\n","ak2.to_csv(\"classifiedByBerttweet_company\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["Doing the same for unseen time"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.367099Z","iopub.status.idle":"2023-12-14T12:50:00.368079Z","shell.execute_reply":"2023-12-14T12:50:00.367870Z","shell.execute_reply.started":"2023-12-14T12:50:00.367839Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['/path/to/file/archive (4)/content-test-company-date.npy',\n"," '/path/to/file/archive (4)/content-test-company-inferred-company.npy',\n"," '/path/to/file/archive (4)/content-test-company-username.npy',\n"," '/path/to/file/archive (4)/content-test-company.npy',\n"," '/path/to/file/archive (4)/content-test-time-date.npy',\n"," '/path/to/file/archive (4)/content-test-time-inferred-company.npy',\n"," '/path/to/file/archive (4)/content-test-time-username.npy',\n"," '/path/to/file/archive (4)/content-test-time.npy',\n"," '/path/to/file/archive (4)/full-content-test-company.npy',\n"," '/path/to/file/archive (4)/full-content-test-time.npy']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["npy_files"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.369607Z","iopub.status.idle":"2023-12-14T12:50:00.370013Z","shell.execute_reply":"2023-12-14T12:50:00.369836Z","shell.execute_reply.started":"2023-12-14T12:50:00.369817Z"},"trusted":true},"outputs":[],"source":["# for time data\n","# Load each .npy file and concatenate them\n","arrays = [np.load(file) for file in npy_files[7:8]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_text = pd.DataFrame(combined_array, columns=column_names)\n","\n","arrays = [np.load(file) for file in npy_files[4:5]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_date = pd.DataFrame(combined_array, columns=column_names)\n","\n","arrays = [np.load(file) for file in npy_files[5:6]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_com = pd.DataFrame(combined_array, columns=column_names)\n","\n","arrays = [np.load(file) for file in npy_files[6:7]]\n","combined_array = np.concatenate(arrays, axis=0)  # Adjust the axis as needed\n","\n","# Convert the combined array to a Pandas DataFrame\n","column_names = [f\"column_{i}\" for i in range(combined_array.shape[1])]\n","df_usr = pd.DataFrame(combined_array, columns=column_names)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:50:00.371402Z","iopub.status.idle":"2023-12-14T12:50:00.371824Z","shell.execute_reply":"2023-12-14T12:50:00.371645Z","shell.execute_reply.started":"2023-12-14T12:50:00.371626Z"},"trusted":true},"outputs":[],"source":["ac=[]\n","ind=[]\n","for index in range(0,10000):\n","    predicted_class = predict_likes_class(clf, index,df_text,df_date,df_com,df_usr)\n","    ind.append(index)\n","    ac.append(predicted_class[0])\n","#     print(f'Predicted Likes Class: {predicted_class[0]}')\n","#     print(f\"actual likes were {df2.iloc[index]['likes']}\")\n","ak2=pd.DataFrame({\"index\":ind,\"classByBertTweetClassifier_time\":ac})\n","ak2.to_csv(\"classifiedByBerttweet_time\",index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4143908,"sourceId":7172055,"sourceType":"datasetVersion"},{"datasetId":4164521,"sourceId":7200238,"sourceType":"datasetVersion"},{"datasetId":4163813,"sourceId":7199171,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
