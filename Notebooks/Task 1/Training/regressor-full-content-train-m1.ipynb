{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3424312b",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc964bac",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-14T13:02:45.641863Z",
     "iopub.status.busy": "2023-12-14T13:02:45.641087Z",
     "iopub.status.idle": "2023-12-14T13:02:46.152116Z",
     "shell.execute_reply": "2023-12-14T13:02:46.150195Z"
    },
    "papermill": {
     "duration": 0.526128,
     "end_time": "2023-12-14T13:02:46.155545",
     "exception": false,
     "start_time": "2023-12-14T13:02:45.629417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_cs\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPool1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f273b5a",
   "metadata": {
    "papermill": {
     "duration": 0.008539,
     "end_time": "2023-12-14T13:02:46.173435",
     "exception": false,
     "start_time": "2023-12-14T13:02:46.164896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Pre Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545f66e",
   "metadata": {},
   "source": [
    "Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5e384f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:08.742966Z",
     "iopub.status.busy": "2023-12-14T13:03:08.741735Z",
     "iopub.status.idle": "2023-12-14T13:03:23.295567Z",
     "shell.execute_reply": "2023-12-14T13:03:23.294534Z"
    },
    "papermill": {
     "duration": 14.567428,
     "end_time": "2023-12-14T13:03:23.298303",
     "exception": false,
     "start_time": "2023-12-14T13:03:08.730875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we had to split our embeddings generator into two parts die to our hardware limitations\n",
    "# Adobe dataset - content\n",
    "\n",
    "# Load your embeddings from the two files\n",
    "embedding_file_1 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_full_content_head_embeddings.npy\"\n",
    "embedding_file_2 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_full_content_tail_embeddings.npy\"\n",
    "\n",
    "embeddings_1 = np.load(embedding_file_1)\n",
    "embeddings_2 = np.load(embedding_file_2)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "e1 = np.concatenate([embeddings_1, embeddings_2], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd5bdd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:23.319504Z",
     "iopub.status.busy": "2023-12-14T13:03:23.318605Z",
     "iopub.status.idle": "2023-12-14T13:03:23.380246Z",
     "shell.execute_reply": "2023-12-14T13:03:23.378754Z"
    },
    "papermill": {
     "duration": 0.076132,
     "end_time": "2023-12-14T13:03:23.384083",
     "exception": false,
     "start_time": "2023-12-14T13:03:23.307951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adobe dataset - likes\n",
    "\n",
    "# Load your embeddings from the two files\n",
    "embedding_file_1 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_head_likes.npy\"\n",
    "embedding_file_2 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_tail_likes.npy\"\n",
    "\n",
    "embeddings_1 = np.load(embedding_file_1)\n",
    "embeddings_2 = np.load(embedding_file_2)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "t1 = np.concatenate([embeddings_1, embeddings_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07e681a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:23.404411Z",
     "iopub.status.busy": "2023-12-14T13:03:23.403951Z",
     "iopub.status.idle": "2023-12-14T13:03:31.858771Z",
     "shell.execute_reply": "2023-12-14T13:03:31.857822Z"
    },
    "papermill": {
     "duration": 8.468412,
     "end_time": "2023-12-14T13:03:31.861724",
     "exception": false,
     "start_time": "2023-12-14T13:03:23.393312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hugging face dataset - content\n",
    "\n",
    "# Load your embeddings from the two files\n",
    "embedding_file_1 = \"/kaggle/input/huggingf-dataset/full-content-huggingf-head.npy\"\n",
    "embedding_file_2 = \"/kaggle/input/huggingf-dataset/full-content-huggingf-tail.npy\"\n",
    "\n",
    "embeddings_1 = np.load(embedding_file_1)\n",
    "embeddings_2 = np.load(embedding_file_2)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "e2 = np.concatenate([embeddings_1, embeddings_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224c9026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:31.882755Z",
     "iopub.status.busy": "2023-12-14T13:03:31.881781Z",
     "iopub.status.idle": "2023-12-14T13:03:31.931792Z",
     "shell.execute_reply": "2023-12-14T13:03:31.930544Z"
    },
    "papermill": {
     "duration": 0.063902,
     "end_time": "2023-12-14T13:03:31.934853",
     "exception": false,
     "start_time": "2023-12-14T13:03:31.870951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hugging face dataset - likes\n",
    "\n",
    "embedding_file_1 = \"/kaggle/input/huggingf-dataset/likes-hugginggf-head.npy\"\n",
    "embedding_file_2 = \"/kaggle/input/huggingf-dataset/likes-hugginggf-tail.npy\"\n",
    "\n",
    "# Load embeddings with allow_pickle=True\n",
    "embeddings_1 = np.load(embedding_file_1, allow_pickle=True)\n",
    "embeddings_2 = np.load(embedding_file_2, allow_pickle=True)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "t2= np.concatenate((embeddings_1, embeddings_2), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeec1490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:31.955529Z",
     "iopub.status.busy": "2023-12-14T13:03:31.955056Z",
     "iopub.status.idle": "2023-12-14T13:03:33.733233Z",
     "shell.execute_reply": "2023-12-14T13:03:33.731939Z"
    },
    "papermill": {
     "duration": 1.792293,
     "end_time": "2023-12-14T13:03:33.736608",
     "exception": false,
     "start_time": "2023-12-14T13:03:31.944315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 20 most followed people datasets\n",
    "\n",
    "# content\n",
    "e3=np.load(\"/kaggle/input/all-embeddings/full-content-add-ds.npy\")\n",
    "\n",
    "# likes\n",
    "t3=np.load(\"/kaggle/input/all-embeddings/likes-add-ds.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb601ec",
   "metadata": {},
   "source": [
    "Creating Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88832dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:34.304390Z",
     "iopub.status.busy": "2023-12-14T13:03:34.303506Z",
     "iopub.status.idle": "2023-12-14T13:03:37.039237Z",
     "shell.execute_reply": "2023-12-14T13:03:37.037748Z"
    },
    "papermill": {
     "duration": 2.749967,
     "end_time": "2023-12-14T13:03:37.042498",
     "exception": false,
     "start_time": "2023-12-14T13:03:34.292531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine embeddings and target values\n",
    "X_train = np.concatenate([e1, e2, e3])\n",
    "y_train = np.concatenate([t1, t2, t3])\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_indices = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle_indices]\n",
    "y_train = y_train[shuffle_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f4b9b",
   "metadata": {},
   "source": [
    "Splitting our Dataset into classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54592a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:37.094270Z",
     "iopub.status.busy": "2023-12-14T13:03:37.093250Z",
     "iopub.status.idle": "2023-12-14T13:05:00.388650Z",
     "shell.execute_reply": "2023-12-14T13:05:00.387497Z"
    },
    "papermill": {
     "duration": 83.309696,
     "end_time": "2023-12-14T13:05:00.392139",
     "exception": false,
     "start_time": "2023-12-14T13:03:37.082443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to combine embeddings and targets\n",
    "train_data = pd.DataFrame({'embeddings': X_train.tolist(), 'targets': y_train})\n",
    "\n",
    "# Filter out rows with non-integer values in the \"targets\" column\n",
    "train_data = train_data[train_data['targets'].astype(str).str.isdigit()]\n",
    "\n",
    "# Convert the \"targets\" column to integers\n",
    "train_data['targets'] = train_data['targets'].astype(int)\n",
    "\n",
    "# Define the class ranges\n",
    "class_ranges = [(0, 100), (100, 1000), (1000, 5000), (5000, 10000), (10000, 100000), (100000, 1000000)]\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames for each class\n",
    "class_dfs = {}\n",
    "\n",
    "# Process data in chunks\n",
    "chunk_size = 10000  # Adjust the chunk size based on your available memory\n",
    "\n",
    "# Create separate DataFrames for each class range\n",
    "for idx, (lower, upper) in enumerate(class_ranges):\n",
    "    class_name = f'class{idx + 1}'\n",
    "    class_dfs[class_name] = pd.DataFrame()\n",
    "\n",
    "    # Process data in chunks\n",
    "    for start_idx in range(0, len(train_data), chunk_size):\n",
    "        end_idx = start_idx + chunk_size\n",
    "\n",
    "        # Filter data based on target range\n",
    "        chunk_df = train_data[(train_data['targets'] >= lower) & (train_data['targets'] <= upper)].iloc[start_idx:end_idx]\n",
    "\n",
    "        # Concatenate to the class DataFrame\n",
    "        class_dfs[class_name] = pd.concat([class_dfs[class_name], chunk_df])\n",
    "\n",
    "# Now class_dfs contains separate DataFrames for each class range\n",
    "# You can access them using keys like class_dfs['class1'], class_dfs['class2'], etc.\n",
    "\n",
    "# Extract embeddings and targets for each class\n",
    "class_embeddings = {}\n",
    "class_targets = {}\n",
    "\n",
    "for class_name, class_df in class_dfs.items():\n",
    "    class_embeddings[class_name] = np.array(class_df['embeddings'].to_list())\n",
    "    class_targets[class_name] = class_df['targets'].values\n",
    "\n",
    "# Now class_embeddings and class_targets contain separate arrays for each class range\n",
    "# You can access them using keys like class_embeddings['class1'], class_targets['class2'], etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e448aea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:05:00.412849Z",
     "iopub.status.busy": "2023-12-14T13:05:00.412373Z",
     "iopub.status.idle": "2023-12-14T13:05:00.419878Z",
     "shell.execute_reply": "2023-12-14T13:05:00.418665Z"
    },
    "papermill": {
     "duration": 0.020236,
     "end_time": "2023-12-14T13:05:00.421937",
     "exception": false,
     "start_time": "2023-12-14T13:05:00.401701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accessing classes\n",
    "class_name = 'class1'\n",
    "embeddings_class1 = class_embeddings[class_name]\n",
    "targets_class1 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class2'\n",
    "embeddings_class2 = class_embeddings[class_name]\n",
    "targets_class2 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class3'\n",
    "embeddings_class3 = class_embeddings[class_name]\n",
    "targets_class3 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class4'\n",
    "embeddings_class4 = class_embeddings[class_name]\n",
    "targets_class4 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class5'\n",
    "embeddings_class5 = class_embeddings[class_name]\n",
    "targets_class5 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class6'\n",
    "embeddings_class6 = class_embeddings[class_name]\n",
    "targets_class6 = class_targets[class_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793fcfe",
   "metadata": {},
   "source": [
    "Training Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a10ea",
   "metadata": {
    "papermill": {
     "duration": 0.009505,
     "end_time": "2023-12-14T13:05:00.586487",
     "exception": false,
     "start_time": "2023-12-14T13:05:00.576982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES : 0 - 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd46d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:05:00.636170Z",
     "iopub.status.busy": "2023-12-14T13:05:00.635306Z",
     "iopub.status.idle": "2023-12-14T13:17:30.777086Z",
     "shell.execute_reply": "2023-12-14T13:17:30.775281Z"
    },
    "papermill": {
     "duration": 750.156099,
     "end_time": "2023-12-14T13:17:30.780645",
     "exception": false,
     "start_time": "2023-12-14T13:05:00.624546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class1 \n",
    "y_train = targets_class1\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model.save(\"0_100_reg.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf4ace",
   "metadata": {
    "papermill": {
     "duration": 1.06785,
     "end_time": "2023-12-14T13:17:32.829673",
     "exception": false,
     "start_time": "2023-12-14T13:17:31.761823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 100-1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b5886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:17:34.895377Z",
     "iopub.status.busy": "2023-12-14T13:17:34.894818Z",
     "iopub.status.idle": "2023-12-14T13:22:43.924782Z",
     "shell.execute_reply": "2023-12-14T13:22:43.923097Z"
    },
    "papermill": {
     "duration": 310.113879,
     "end_time": "2023-12-14T13:22:43.927687",
     "exception": false,
     "start_time": "2023-12-14T13:17:33.813808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class2 \n",
    "y_train = targets_class2\n",
    "\n",
    "\n",
    "model1 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model1.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model1.save(\"100_1000_reg.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56322448",
   "metadata": {
    "papermill": {
     "duration": 1.277828,
     "end_time": "2023-12-14T13:22:46.595017",
     "exception": false,
     "start_time": "2023-12-14T13:22:45.317189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 1000-5000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a373a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:22:49.369582Z",
     "iopub.status.busy": "2023-12-14T13:22:49.369078Z",
     "iopub.status.idle": "2023-12-14T13:23:50.940713Z",
     "shell.execute_reply": "2023-12-14T13:23:50.939353Z"
    },
    "papermill": {
     "duration": 62.960528,
     "end_time": "2023-12-14T13:23:50.943212",
     "exception": false,
     "start_time": "2023-12-14T13:22:47.982684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class3 \n",
    "y_train = targets_class3\n",
    "\n",
    "\n",
    "model2 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model2.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model2.save(\"1000_5000_reg.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a35ad4",
   "metadata": {
    "papermill": {
     "duration": 1.453529,
     "end_time": "2023-12-14T13:23:53.847022",
     "exception": false,
     "start_time": "2023-12-14T13:23:52.393493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 5000-10000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d4f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:23:56.629142Z",
     "iopub.status.busy": "2023-12-14T13:23:56.628636Z",
     "iopub.status.idle": "2023-12-14T13:25:47.278108Z",
     "shell.execute_reply": "2023-12-14T13:25:47.276638Z"
    },
    "papermill": {
     "duration": 112.088876,
     "end_time": "2023-12-14T13:25:47.280650",
     "exception": false,
     "start_time": "2023-12-14T13:23:55.191774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class4 \n",
    "y_train = targets_class4\n",
    "\n",
    "\n",
    "model3 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model3.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model3.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model3.save(\"5000_10000_reg.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e40041",
   "metadata": {
    "papermill": {
     "duration": 1.569254,
     "end_time": "2023-12-14T13:25:50.326424",
     "exception": false,
     "start_time": "2023-12-14T13:25:48.757170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 10000-100000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d01efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:25:53.591971Z",
     "iopub.status.busy": "2023-12-14T13:25:53.591492Z",
     "iopub.status.idle": "2023-12-14T13:30:32.505111Z",
     "shell.execute_reply": "2023-12-14T13:30:32.503696Z"
    },
    "papermill": {
     "duration": 280.508122,
     "end_time": "2023-12-14T13:30:32.507702",
     "exception": false,
     "start_time": "2023-12-14T13:25:51.999580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class5 \n",
    "y_train = targets_class5\n",
    "\n",
    "model4 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model4.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model4.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "model4.save(\"10000_100000_reg.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb441e",
   "metadata": {
    "papermill": {
     "duration": 1.860167,
     "end_time": "2023-12-14T13:30:36.416246",
     "exception": false,
     "start_time": "2023-12-14T13:30:34.556079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 100000-1000000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bbdde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:30:40.308823Z",
     "iopub.status.busy": "2023-12-14T13:30:40.307220Z",
     "iopub.status.idle": "2023-12-14T13:31:01.418353Z",
     "shell.execute_reply": "2023-12-14T13:31:01.416849Z"
    },
    "papermill": {
     "duration": 23.053798,
     "end_time": "2023-12-14T13:31:01.421417",
     "exception": false,
     "start_time": "2023-12-14T13:30:38.367619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class6\n",
    "y_train = targets_class6\n",
    "\n",
    "\n",
    "model5 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model5.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model5.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model5.save(\"100000_1000000_reg.h5\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4091163,
     "sourceId": 7097978,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4125298,
     "sourceId": 7148774,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4128137,
     "sourceId": 7150035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4129567,
     "sourceId": 7152081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4133094,
     "sourceId": 7156664,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4138712,
     "sourceId": 7164742,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4144344,
     "sourceId": 7172653,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4159000,
     "sourceId": 7192285,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1705.153656,
   "end_time": "2023-12-14T13:31:07.279884",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-14T13:02:42.126228",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
