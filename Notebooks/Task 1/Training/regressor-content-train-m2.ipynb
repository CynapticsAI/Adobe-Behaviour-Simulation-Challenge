{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a2676",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-14T12:29:41.461973Z",
     "iopub.status.busy": "2023-12-14T12:29:41.461413Z",
     "iopub.status.idle": "2023-12-14T12:29:41.970635Z",
     "shell.execute_reply": "2023-12-14T12:29:41.968741Z"
    },
    "papermill": {
     "duration": 0.52794,
     "end_time": "2023-12-14T12:29:41.974323",
     "exception": false,
     "start_time": "2023-12-14T12:29:41.446383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_cs\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPool1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21861da",
   "metadata": {
    "papermill": {
     "duration": 0.011944,
     "end_time": "2023-12-14T12:29:41.998951",
     "exception": false,
     "start_time": "2023-12-14T12:29:41.987007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Pre Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610019a",
   "metadata": {},
   "source": [
    "Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f0d94b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:30:05.204833Z",
     "iopub.status.busy": "2023-12-14T12:30:05.204358Z",
     "iopub.status.idle": "2023-12-14T12:30:16.073283Z",
     "shell.execute_reply": "2023-12-14T12:30:16.071785Z"
    },
    "papermill": {
     "duration": 10.885757,
     "end_time": "2023-12-14T12:30:16.076357",
     "exception": false,
     "start_time": "2023-12-14T12:30:05.190600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we had to split our embeddings generator into two parts die to our hardware limitations\n",
    "# Adobe dataset - content\n",
    "\n",
    "# Load your embeddings from the two files\n",
    "embedding_file_1 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_content_embeddings_head.npy\"\n",
    "embedding_file_2 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_content_embeddings_tail.npy\"\n",
    "\n",
    "embeddings_1 = np.load(embedding_file_1)\n",
    "embeddings_2 = np.load(embedding_file_2)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "e1 = np.concatenate([embeddings_1, embeddings_2], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2103cf2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:30:16.102920Z",
     "iopub.status.busy": "2023-12-14T12:30:16.102404Z",
     "iopub.status.idle": "2023-12-14T12:30:16.156870Z",
     "shell.execute_reply": "2023-12-14T12:30:16.155502Z"
    },
    "papermill": {
     "duration": 0.071482,
     "end_time": "2023-12-14T12:30:16.159723",
     "exception": false,
     "start_time": "2023-12-14T12:30:16.088241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adobe dataset - likes\n",
    "\n",
    "# Load your embeddings from the two files\n",
    "embedding_file_1 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_head_likes.npy\"\n",
    "embedding_file_2 = \"/kaggle/input/embeddings-adobe-bertweet/bertweet_tail_likes.npy\"\n",
    "\n",
    "embeddings_1 = np.load(embedding_file_1)\n",
    "embeddings_2 = np.load(embedding_file_2)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "t1 = np.concatenate([embeddings_1, embeddings_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970537f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:30:16.193073Z",
     "iopub.status.busy": "2023-12-14T12:30:16.192523Z",
     "iopub.status.idle": "2023-12-14T12:30:23.452971Z",
     "shell.execute_reply": "2023-12-14T12:30:23.451458Z"
    },
    "papermill": {
     "duration": 7.282213,
     "end_time": "2023-12-14T12:30:23.456508",
     "exception": false,
     "start_time": "2023-12-14T12:30:16.174295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hugging face dataset - content\n",
    "\n",
    "# Load your embeddings from the two files\n",
    "embedding_file_1 = \"/kaggle/input/huggingf-embeddings-content/content-huggingf-head.npy\"\n",
    "embedding_file_2 = \"/kaggle/input/huggingf-embeddings-content/content-huggingf-tail.npy\"\n",
    "\n",
    "embeddings_1 = np.load(embedding_file_1)\n",
    "embeddings_2 = np.load(embedding_file_2)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "e2 = np.concatenate([embeddings_1, embeddings_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649535ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:30:23.484475Z",
     "iopub.status.busy": "2023-12-14T12:30:23.483049Z",
     "iopub.status.idle": "2023-12-14T12:30:23.525919Z",
     "shell.execute_reply": "2023-12-14T12:30:23.524770Z"
    },
    "papermill": {
     "duration": 0.060043,
     "end_time": "2023-12-14T12:30:23.529150",
     "exception": false,
     "start_time": "2023-12-14T12:30:23.469107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hugging face dataset - likes\n",
    "\n",
    "embedding_file_1 = \"/kaggle/input/huggingf-embeddings-content/likes-hugginggf-head (1).npy\"\n",
    "embedding_file_2 = \"/kaggle/input/huggingf-embeddings-content/likes-hugginggf-tail (1).npy\"\n",
    "\n",
    "# Load embeddings with allow_pickle=True\n",
    "embeddings_1 = np.load(embedding_file_1, allow_pickle=True)\n",
    "embeddings_2 = np.load(embedding_file_2, allow_pickle=True)\n",
    "\n",
    "# Concatenate the embeddings along the appropriate axis (axis=0 for vertical concatenation)\n",
    "t2= np.concatenate((embeddings_1, embeddings_2), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87b4c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:30:23.557139Z",
     "iopub.status.busy": "2023-12-14T12:30:23.555688Z",
     "iopub.status.idle": "2023-12-14T12:30:25.376431Z",
     "shell.execute_reply": "2023-12-14T12:30:25.374968Z"
    },
    "papermill": {
     "duration": 1.838436,
     "end_time": "2023-12-14T12:30:25.379980",
     "exception": false,
     "start_time": "2023-12-14T12:30:23.541544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 20 most followed people datasets\n",
    "\n",
    "# content\n",
    "e3=np.load(\"/kaggle/input/new-dataset-bertweet-embeddings/bertweet2_test_embeddings.npy\")\n",
    "\n",
    "# likes\n",
    "t3=np.load(\"/kaggle/input/new-dataset-bertweet-embeddings/bertweet2_test_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237adc10",
   "metadata": {},
   "source": [
    "Creating Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a8e5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:30:25.954207Z",
     "iopub.status.busy": "2023-12-14T12:30:25.953365Z",
     "iopub.status.idle": "2023-12-14T12:30:27.750024Z",
     "shell.execute_reply": "2023-12-14T12:30:27.748718Z"
    },
    "papermill": {
     "duration": 1.814155,
     "end_time": "2023-12-14T12:30:27.753395",
     "exception": false,
     "start_time": "2023-12-14T12:30:25.939240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine embeddings and target values\n",
    "X_train = np.concatenate([e1, e2, e3])\n",
    "y_train = np.concatenate([t1, t2, t3])\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_indices = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle_indices]\n",
    "y_train = y_train[shuffle_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec49d96",
   "metadata": {},
   "source": [
    "Creating Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3157de59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:30:27.819984Z",
     "iopub.status.busy": "2023-12-14T12:30:27.819507Z",
     "iopub.status.idle": "2023-12-14T12:32:01.999268Z",
     "shell.execute_reply": "2023-12-14T12:32:01.997721Z"
    },
    "papermill": {
     "duration": 94.197457,
     "end_time": "2023-12-14T12:32:02.002519",
     "exception": false,
     "start_time": "2023-12-14T12:30:27.805062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming e_train, t_train are your train embeddings and targets\n",
    "\n",
    "# Create a DataFrame to combine embeddings and targets\n",
    "train_data = pd.DataFrame({'embeddings': X_train.tolist(), 'targets': y_train})\n",
    "\n",
    "# Filter out rows with non-integer values in the \"targets\" column\n",
    "train_data = train_data[train_data['targets'].astype(str).str.isdigit()]\n",
    "\n",
    "# Convert the \"targets\" column to integers\n",
    "train_data['targets'] = train_data['targets'].astype(int)\n",
    "\n",
    "# Define the class ranges\n",
    "class_ranges = [(0, 100), (100, 1000), (1000, 5000), (5000, 10000), (10000, 100000), (100000, 1000000)]\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames for each class\n",
    "class_dfs = {}\n",
    "\n",
    "# Process data in chunks\n",
    "chunk_size = 10000  # Adjust the chunk size based on your available memory\n",
    "\n",
    "# Create separate DataFrames for each class range\n",
    "for idx, (lower, upper) in enumerate(class_ranges):\n",
    "    class_name = f'class{idx + 1}'\n",
    "    class_dfs[class_name] = pd.DataFrame()\n",
    "\n",
    "    # Process data in chunks\n",
    "    for start_idx in range(0, len(train_data), chunk_size):\n",
    "        end_idx = start_idx + chunk_size\n",
    "\n",
    "        # Filter data based on target range\n",
    "        chunk_df = train_data[(train_data['targets'] >= lower) & (train_data['targets'] <= upper)].iloc[start_idx:end_idx]\n",
    "\n",
    "        # Concatenate to the class DataFrame\n",
    "        class_dfs[class_name] = pd.concat([class_dfs[class_name], chunk_df])\n",
    "\n",
    "# Now class_dfs contains separate DataFrames for each class range\n",
    "# You can access them using keys like class_dfs['class1'], class_dfs['class2'], etc.\n",
    "\n",
    "# Extract embeddings and targets for each class\n",
    "class_embeddings = {}\n",
    "class_targets = {}\n",
    "\n",
    "for class_name, class_df in class_dfs.items():\n",
    "    class_embeddings[class_name] = np.array(class_df['embeddings'].to_list())\n",
    "    class_targets[class_name] = class_df['targets'].values\n",
    "\n",
    "# Now class_embeddings and class_targets contain separate arrays for each class range\n",
    "# You can access them using keys like class_embeddings['class1'], class_targets['class2'], etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d8d69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:32:02.030391Z",
     "iopub.status.busy": "2023-12-14T12:32:02.029880Z",
     "iopub.status.idle": "2023-12-14T12:32:02.038724Z",
     "shell.execute_reply": "2023-12-14T12:32:02.037189Z"
    },
    "papermill": {
     "duration": 0.026056,
     "end_time": "2023-12-14T12:32:02.041347",
     "exception": false,
     "start_time": "2023-12-14T12:32:02.015291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accessing class 1\n",
    "class_name = 'class1'\n",
    "embeddings_class1 = class_embeddings[class_name]\n",
    "targets_class1 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class2'\n",
    "embeddings_class2 = class_embeddings[class_name]\n",
    "targets_class2 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class3'\n",
    "embeddings_class3 = class_embeddings[class_name]\n",
    "targets_class3 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class4'\n",
    "embeddings_class4 = class_embeddings[class_name]\n",
    "targets_class4 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class5'\n",
    "embeddings_class5 = class_embeddings[class_name]\n",
    "targets_class5 = class_targets[class_name]\n",
    "\n",
    "class_name = 'class6'\n",
    "embeddings_class6 = class_embeddings[class_name]\n",
    "targets_class6 = class_targets[class_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa04b48",
   "metadata": {},
   "source": [
    "Training the regressors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6660fd8",
   "metadata": {
    "papermill": {
     "duration": 0.012556,
     "end_time": "2023-12-14T12:32:02.253689",
     "exception": false,
     "start_time": "2023-12-14T12:32:02.241133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES : 0 - 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c62eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:32:02.317174Z",
     "iopub.status.busy": "2023-12-14T12:32:02.316756Z",
     "iopub.status.idle": "2023-12-14T12:47:33.375596Z",
     "shell.execute_reply": "2023-12-14T12:47:33.373750Z"
    },
    "papermill": {
     "duration": 931.075978,
     "end_time": "2023-12-14T12:47:33.378415",
     "exception": false,
     "start_time": "2023-12-14T12:32:02.302437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class1 \n",
    "y_train = targets_class1\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model.save(\"0_100_reg.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9668b2",
   "metadata": {
    "papermill": {
     "duration": 1.794658,
     "end_time": "2023-12-14T12:47:36.970452",
     "exception": false,
     "start_time": "2023-12-14T12:47:35.175794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 100-1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27675df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:47:40.446838Z",
     "iopub.status.busy": "2023-12-14T12:47:40.446347Z",
     "iopub.status.idle": "2023-12-14T12:54:05.860724Z",
     "shell.execute_reply": "2023-12-14T12:54:05.857924Z"
    },
    "papermill": {
     "duration": 387.087836,
     "end_time": "2023-12-14T12:54:05.864300",
     "exception": false,
     "start_time": "2023-12-14T12:47:38.776464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class2 \n",
    "y_train = targets_class2\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model.save(\"100_1000_reg.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7ba99",
   "metadata": {
    "papermill": {
     "duration": 2.424854,
     "end_time": "2023-12-14T12:54:10.636418",
     "exception": false,
     "start_time": "2023-12-14T12:54:08.211564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 1000-5000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d0c98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:54:15.485719Z",
     "iopub.status.busy": "2023-12-14T12:54:15.485207Z",
     "iopub.status.idle": "2023-12-14T12:55:11.426806Z",
     "shell.execute_reply": "2023-12-14T12:55:11.425495Z"
    },
    "papermill": {
     "duration": 58.366019,
     "end_time": "2023-12-14T12:55:11.429448",
     "exception": false,
     "start_time": "2023-12-14T12:54:13.063429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class3 \n",
    "y_train = targets_class3\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model.save(\"1000_5000_reg.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d8da5",
   "metadata": {
    "papermill": {
     "duration": 2.54645,
     "end_time": "2023-12-14T12:55:16.508230",
     "exception": false,
     "start_time": "2023-12-14T12:55:13.961780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 5000-10000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba5853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:55:21.532367Z",
     "iopub.status.busy": "2023-12-14T12:55:21.531825Z",
     "iopub.status.idle": "2023-12-14T12:58:10.907178Z",
     "shell.execute_reply": "2023-12-14T12:58:10.905097Z"
    },
    "papermill": {
     "duration": 171.888967,
     "end_time": "2023-12-14T12:58:10.910560",
     "exception": false,
     "start_time": "2023-12-14T12:55:19.021593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class4 \n",
    "y_train = targets_class4\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model.save(\"5000_10000_reg.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaaec4",
   "metadata": {
    "papermill": {
     "duration": 2.825971,
     "end_time": "2023-12-14T12:58:16.409791",
     "exception": false,
     "start_time": "2023-12-14T12:58:13.583820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 10000-100000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fbb8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T12:58:22.206482Z",
     "iopub.status.busy": "2023-12-14T12:58:22.205910Z",
     "iopub.status.idle": "2023-12-14T13:03:33.799547Z",
     "shell.execute_reply": "2023-12-14T13:03:33.797782Z"
    },
    "papermill": {
     "duration": 314.557485,
     "end_time": "2023-12-14T13:03:33.802927",
     "exception": false,
     "start_time": "2023-12-14T12:58:19.245442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class5 \n",
    "y_train = targets_class5\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "model.save(\"10000_100000_reg.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e4c72",
   "metadata": {
    "papermill": {
     "duration": 3.325989,
     "end_time": "2023-12-14T13:03:40.476531",
     "exception": false,
     "start_time": "2023-12-14T13:03:37.150542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LIKES: 100000-1000000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390c446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T13:03:47.191064Z",
     "iopub.status.busy": "2023-12-14T13:03:47.189944Z",
     "iopub.status.idle": "2023-12-14T13:04:39.295526Z",
     "shell.execute_reply": "2023-12-14T13:04:39.293892Z"
    },
    "papermill": {
     "duration": 55.628691,
     "end_time": "2023-12-14T13:04:39.298457",
     "exception": false,
     "start_time": "2023-12-14T13:03:43.669766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = embeddings_class6\n",
    "y_train = targets_class6\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=2),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='model-regression.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Training started\")\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "print(\"Training completed\")\n",
    "print(\"Time required: \" + str(time.time() - start))\n",
    "\n",
    "model.save(\"100000_1000000_reg.h5\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4091163,
     "sourceId": 7097978,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4125298,
     "sourceId": 7148774,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4128137,
     "sourceId": 7150035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4129567,
     "sourceId": 7152081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4133094,
     "sourceId": 7156664,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4138712,
     "sourceId": 7164742,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4144344,
     "sourceId": 7172653,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4159058,
     "sourceId": 7192357,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2109.313965,
   "end_time": "2023-12-14T13:04:46.937559",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-14T12:29:37.623594",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
