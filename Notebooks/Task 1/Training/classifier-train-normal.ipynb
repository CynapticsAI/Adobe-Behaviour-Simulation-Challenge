{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97769d3",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the necessaring libs\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dbf4f9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-07T17:20:53.280004Z",
     "iopub.status.busy": "2023-12-07T17:20:53.279463Z",
     "iopub.status.idle": "2023-12-07T17:20:53.776035Z",
     "shell.execute_reply": "2023-12-07T17:20:53.774493Z"
    },
    "papermill": {
     "duration": 0.507021,
     "end_time": "2023-12-07T17:20:53.779162",
     "exception": false,
     "start_time": "2023-12-07T17:20:53.272141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bst-300k/behaviour_simulation_train.csv\n",
      "/kaggle/input/bst-300k-above100k/DataInLakhs (1).csv\n",
      "/kaggle/input/similarity2/final.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c3b333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T17:21:49.329168Z",
     "iopub.status.busy": "2023-12-07T17:21:49.328677Z",
     "iopub.status.idle": "2023-12-07T17:21:49.335420Z",
     "shell.execute_reply": "2023-12-07T17:21:49.334614Z"
    },
    "papermill": {
     "duration": 0.018695,
     "end_time": "2023-12-07T17:21:49.337672",
     "exception": false,
     "start_time": "2023-12-07T17:21:49.318977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_likes_classes(likes):\n",
    "    if likes <= 100:\n",
    "        return '0-100'\n",
    "    elif 100 < likes <= 1000:\n",
    "        return '100-1000'\n",
    "    elif 1000 < likes <= 5000:\n",
    "        return '1000-5000'\n",
    "    elif 5000 < likes <= 10000:\n",
    "        return '5000-10000'\n",
    "    elif 10000 < likes <= 100000:\n",
    "        return '10000-100000'\n",
    "    else:\n",
    "        return '100000-1000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "003b7758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T17:21:49.356200Z",
     "iopub.status.busy": "2023-12-07T17:21:49.355343Z",
     "iopub.status.idle": "2023-12-07T17:21:59.481869Z",
     "shell.execute_reply": "2023-12-07T17:21:59.480448Z"
    },
    "papermill": {
     "duration": 10.139261,
     "end_time": "2023-12-07T17:21:59.485241",
     "exception": false,
     "start_time": "2023-12-07T17:21:49.345980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading train data set file\n",
    "data_path = '/kaggle/input/bst-300k/behaviour_simulation_train.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c99b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T17:21:59.503843Z",
     "iopub.status.busy": "2023-12-07T17:21:59.503381Z",
     "iopub.status.idle": "2023-12-07T17:22:03.176028Z",
     "shell.execute_reply": "2023-12-07T17:22:03.174584Z"
    },
    "papermill": {
     "duration": 3.685445,
     "end_time": "2023-12-07T17:22:03.178993",
     "exception": false,
     "start_time": "2023-12-07T17:21:59.493548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/bst-300k/behaviour_simulation_train.csv'\n",
    "df2 = pd.read_csv(data_path)\n",
    "df2['Likes_Class'] = df2['likes'].apply(create_likes_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a672ce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T17:22:03.197682Z",
     "iopub.status.busy": "2023-12-07T17:22:03.197217Z",
     "iopub.status.idle": "2023-12-07T22:10:36.438447Z",
     "shell.execute_reply": "2023-12-07T22:10:36.433673Z"
    },
    "papermill": {
     "duration": 17313.291791,
     "end_time": "2023-12-07T22:10:36.478938",
     "exception": false,
     "start_time": "2023-12-07T17:22:03.187147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  content  inferred company  \\\n",
      "0       Spend your weekend morning with a Ham, Egg, an...       tim hortons   \n",
      "1       Watch rapper <mention> freestyle for over an H...       independent   \n",
      "2       Canadian Armenian community demands ban on mil...               cbc   \n",
      "3       1st in Europe to be devastated by COVID-19, It...          williams   \n",
      "4       Congratulations to Pauletha Butts of <mention>...       independent   \n",
      "...                                                   ...               ...   \n",
      "289995  Cowboys fans, respond with a picture of how yo...           cameron   \n",
      "289996  The Czech Jet TAKES OFF!\\n\\nCongratulations <m...  turkish airlines   \n",
      "289997  Taylor Swift's second surprise album of 2020, ...               cnn   \n",
      "289998  [TOMORROW] Mo G &amp; <mention> are celebratin...              sabc   \n",
      "289999  âœˆðŸ’ª\\nJuntos Somos MÃ¡s Grandes!\\n\\nWe're proud t...  turkish airlines   \n",
      "\n",
      "                       date         username  \n",
      "0       2020-12-12 00:47:00     TimHortonsPH  \n",
      "1       2018-06-30 10:04:20        IndyMusic  \n",
      "2       2020-09-29 19:47:28        CBCCanada  \n",
      "3       2020-10-01 11:40:09   MKWilliamsRome  \n",
      "4       2018-10-19 14:30:46            BGISD  \n",
      "...                     ...              ...  \n",
      "289995  2020-11-27 00:51:28  ScooterMagruder  \n",
      "289996  2019-05-18 19:59:05       EuroLeague  \n",
      "289997  2020-12-11 12:04:19              CNN  \n",
      "289998  2018-01-12 12:29:31        METROFMSA  \n",
      "289999  2019-08-02 19:26:31  TurkishAirlines  \n",
      "\n",
      "[290000 rows x 4 columns]\n",
      "Classification Accuracy: 95.88%\n"
     ]
    }
   ],
   "source": [
    "data_path = '/kaggle/input/bst-300k/behaviour_simulation_train.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# making a split for test-train\n",
    "df=df[:290000]\n",
    "\n",
    "\n",
    "# Load the spaCy model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Create a new column 'Likes_Class' with classes based on the number of likes\n",
    "df['Likes_Class'] = df['likes'].apply(create_likes_classes)\n",
    "\n",
    "# Split the data into training and testing sets for classification\n",
    "X_classification = df[['content', 'inferred company', 'date', 'username']]  # Include 'Date', 'Company', and 'Label'\n",
    "y_classification = df['Likes_Class']\n",
    "\n",
    "# Get word embeddings for the 'Content' column\n",
    "X_text_embeddings = []\n",
    "X_usr_embeddings = []\n",
    "X_com_embeddings = []\n",
    "X_date_embeddings = []\n",
    "print(X_classification)\n",
    "\n",
    "for content in X_classification['content']:\n",
    "    doc = nlp(content)\n",
    "    # Average word vectors to get a single vector for the entire text\n",
    "    content_embedding = doc.vector\n",
    "    X_text_embeddings.append(content_embedding)\n",
    "for usr in X_classification['username']:\n",
    "    doc = nlp(usr)\n",
    "    # Average word vectors to get a single vector for the entire text\n",
    "    content_embedding = doc.vector\n",
    "    X_usr_embeddings.append(content_embedding)    \n",
    "for c in X_classification['inferred company']:\n",
    "    doc = nlp(c)\n",
    "    # Average word vectors to get a single vector for the entire text\n",
    "    content_embedding = doc.vector\n",
    "    X_com_embeddings.append(content_embedding)     \n",
    "    \n",
    "for c in X_classification['date']:\n",
    "    doc = nlp(c)\n",
    "    # Average word vectors to get a single vector for the entire text\n",
    "    content_embedding = doc.vector\n",
    "    X_date_embeddings.append(content_embedding)     \n",
    "\n",
    "X_text_embeddings_df = pd.DataFrame(X_text_embeddings)\n",
    "X_usr_embeddings_df = pd.DataFrame(X_usr_embeddings)\n",
    "X_com_embeddings_df = pd.DataFrame(X_com_embeddings)\n",
    "X_date_embeddings_df = pd.DataFrame(X_date_embeddings)\n",
    "\n",
    "# Combine embeddings with other features\n",
    "\n",
    "# X_combined = pd.concat([X_classification.drop('content', axis=1), X_text_embeddings_df], axis=1)\n",
    "# X_combined2 = pd.concat([X_combined.drop('date', axis=1), X_date_embeddings_df], axis=1)\n",
    "X_combined3 = pd.concat([ X_text_embeddings_df, X_usr_embeddings_df, X_com_embeddings_df, X_date_embeddings_df], axis=1)\n",
    "# X_combined = pd.concat([X_combined.drop('inferred company', axis=1), X_com_embeddings_df], axis=1)\n",
    "# X_combined = pd.concat([X_combined.drop('username', axis=1), X_usr_embeddings_df], axis=1)\n",
    "\n",
    "# oversampling the data\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_combined3, y_classification)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = RandomForestClassifier(n_estimators=200,max_depth=50,class_weight='balanced', random_state=46)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "joblib.dump(classifier, 'random_forest_classifier_bst-300k.joblib')\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Classification Accuracy: {accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b499424",
   "metadata": {},
   "source": [
    "Code to predict on the test split we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae1506f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T22:10:36.610681Z",
     "iopub.status.busy": "2023-12-07T22:10:36.604986Z",
     "iopub.status.idle": "2023-12-07T22:10:42.323415Z",
     "shell.execute_reply": "2023-12-07T22:10:42.321623Z"
    },
    "papermill": {
     "duration": 5.790363,
     "end_time": "2023-12-07T22:10:42.326619",
     "exception": false,
     "start_time": "2023-12-07T22:10:36.536256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 183\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 662\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 1864\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 58\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 7\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 324\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 954\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 5632\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 55\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 4\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 100\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 112\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 134\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 3\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 17\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 268\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 883\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 90\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 336\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 60\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 6\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 5938\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 512\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 3\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 49\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 55\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 2\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 146\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 7\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 447\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1171\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 4\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 754\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 14\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 917\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 2\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 79\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 274\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 196\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 102\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 711\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 77\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 2057\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 2\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 139\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 143\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 605\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 4\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 154\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 12\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 29\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 339\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 119\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 7\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 788\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 5497\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 306\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 21\n",
      "Predicted Class Label for Index 290000: 5000-10000\n",
      " actual likes are 8775\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 1019\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 154\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 10077\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 3\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 1587\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1220\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 144\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 172\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 0\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 4\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 4\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 15\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 185\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 42\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 796\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 48\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 1120\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 3\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 114\n",
      "Predicted Class Label for Index 290000: 1000-5000\n",
      " actual likes are 959\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 12\n",
      "Predicted Class Label for Index 290000: 100-1000\n",
      " actual likes are 130\n",
      "Predicted Class Label for Index 290000: 0-100\n",
      " actual likes are 1\n"
     ]
    }
   ],
   "source": [
    "def predict_likes_class(index, dataframe, text_model, usr_model, com_model, date_model, classifier):\n",
    "    # Extract the relevant information for the given index\n",
    "    content = dataframe.loc[index, 'content']\n",
    "    username = dataframe.loc[index, 'username']\n",
    "    company = dataframe.loc[index, 'inferred company']\n",
    "    date = dataframe.loc[index, 'date']\n",
    "\n",
    "    # Get word embeddings for the input features\n",
    "    content_embedding = text_model(content).vector\n",
    "    usr_embedding = usr_model(username).vector\n",
    "    com_embedding = com_model(company).vector\n",
    "    date_embedding = date_model(date).vector\n",
    "\n",
    "    # Combine the embeddings with other features\n",
    "    input_features = np.concatenate([content_embedding, usr_embedding, com_embedding, date_embedding], axis=None)\n",
    "\n",
    "    # Make the prediction using the trained classifier\n",
    "    predicted_class = classifier.predict([input_features])[0]\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'index_to_predict' with the actual index you want to predict\n",
    "index_to_predict=290000\n",
    "final=100\n",
    "for i in range(index_to_predict,index_to_predict+final): \n",
    "    predicted_class_label = predict_likes_class(i, df2, nlp, nlp, nlp, nlp, classifier)\n",
    "\n",
    "    print(f\"Predicted Class Label for Index {index_to_predict}: {predicted_class_label}\\n actual likes are {df2.iloc[i]['likes']}\" )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4091163,
     "sourceId": 7097978,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4106107,
     "sourceId": 7119356,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4119825,
     "sourceId": 7138717,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17396.851833,
   "end_time": "2023-12-07T22:10:46.224400",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-07T17:20:49.372567",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
